{
 "metadata": {
  "name": "",
  "signature": "sha256:128a620ca4c7a10f5d9b5808644b9dd7fd43a3676d2ee0d7833293e02ca99d86"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import imaginet.task as task"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5005)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Loading models\n",
      "\n",
      "Load models trained on flickr8k captions (clean and noisy)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_clean = task.load(\"/home/gchrupala/reimaginet/examples/audioviz/mfcc-flickr8k.zip\")\n",
      "model_noisy = task.load(\"/home/gchrupala/reimaginet/examples/audioviz/noisy-mfcc-flickr8k.zip\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Loading data\n",
      "\n",
      "Load flickr8k data. This relies on the following files in the reimaginet directory:\n",
      "```\n",
      "data/flickr8k/dataset.json\n",
      "data/flickr8k/vgg_feats.mat\n",
      "data/flickr8k/dataset.mfcc.npy\n",
      "data/flickr8k/dataset.noisy.mfcc.npy\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import imaginet.data_provider as dp\n",
      "\n",
      "# clean speech\n",
      "prov_clean = dp.getDataProvider('flickr8k', root='/home/gchrupala/reimaginet', audio_kind='mfcc')\n",
      "# noisy speech\n",
      "prov_noisy = dp.getDataProvider('flickr8k', root='/home/gchrupala/reimaginet', audio_kind='noisy.mfcc')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Could not read file /home/gchrupala/reimaginet/data/flickr8k/dataset.ipa.jsonl.gz: IPA transcription not available\n",
        "Could not read file /home/gchrupala/reimaginet/data/flickr8k/dataset.ipa.jsonl.gz: IPA transcription not available\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Grab all the validation sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_val_clean = list(prov_clean.iterSentences(split='val'))\n",
      "print sent_val_clean[0]['raw']\n",
      "print sent_val_clean[0]['audio'].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the boy laying face down on a skateboard is being pushed along the ground by another boy .\n",
        "(587, 13)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Running the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run some sentences through the model and get the sentence embeddings. The function `a4.encode_sentences` is pretty fast as runs audios through the model in batches (to do so it will pad sentences in a batch to the same number of timesteps). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import imaginet.defn.audiovis4 as a4\n",
      "\n",
      "data = [ numpy.asarray(sent['audio'], dtype='float32') for sent in sent_val_clean ]\n",
      "embeddings = a4.encode_sentences(model_clean, data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "embeddings.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "(5000, 1024)"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also get the intermediate representations (states of hidden layers). Note that for this particular model the states of the top hidden layer at the last timestep are **NOT** equal to the sentence embeddings, as there is an additional attention-like lookback operation to get the final embeddings.\n",
      "\n",
      "The function `a4.layer_states` is slow because it run each audio through the model one by one, without padding."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states = a4.layer_states(model_clean, data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "(198, 2, 1024)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The three axes are: (timestep, layer, unit)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Generating audio and extracting features \n",
      "\n",
      "**TODO**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}